{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_outliers(df):\n",
    "    IQR = df.quantile(0.75) - df.quantile(0.25)\n",
    "    Number = ((df > (df.quantile(0.75)+(1.5*IQR))) | (df < (df.quantile(0.25)-(1.5*IQR)))).sum()\n",
    "    Outlier = ((df > (df.quantile(0.75)+(1.5*IQR))) | (df < (df.quantile(0.25)-(1.5*IQR))))\n",
    "    dataframe = pd.DataFrame({\"Number of Outliers\":Number})\n",
    "    return (dataframe.sort_values(by=\"Number of Outliers\",ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_null_values(df):\n",
    "    null_value_features=list(df.isnull().sum()[df.isnull().sum()!=0].index)\n",
    "    perc_null=(df[null_value_features].isnull().sum()/df.shape[0])*100\n",
    "    perc_null=pd.DataFrame(perc_null)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(x=perc_null.index,y=perc_null[0],order=perc_null.sort_values(by=0,ascending=False).index)\n",
    "    for (i,j) in enumerate(perc_null.sort_values(by=0,ascending=False)[0].values):\n",
    "        plt.text(x=i-0.3,y=j+0.7,s=\"{:.1f}%\".format(j),color=\"black\",fontsize=10,fontweight=\"bold\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.title('Percentage of null values in the features having null values',fontdict={'fontsize':14,'weight':'bold'})\n",
    "    plt.xlabel('Features',fontdict={'fontsize':10,'weight':'bold'})\n",
    "    plt.ylabel('Null value percentage',fontdict={'fontsize':10,'weight':'bold'})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_unique_values(df):\n",
    "    data = []\n",
    "    for i in df.columns:\n",
    "         data.append(df[i].nunique())\n",
    "    s=pd.DataFrame({\"Name of column\":df.columns,\"Number of unique values\":data})\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using subplots to make multiple boxplots and distplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(nrows=2,ncols=3,figsize=(15,10))\n",
    "a=sns.distplot(pima[\"Glucose\"],kde=False,ax=axes[0,0])\n",
    "a.set_title(\"Histogram of Glucose\",fontsize=14)\n",
    "b=sns.distplot(pima[\"BloodPressure\"],kde=False,ax=axes[0,1])\n",
    "b.set_title(\"Histogram of BloodPressure\",fontsize=14)\n",
    "c=sns.distplot(pima[\"BMI\"],kde=False,ax=axes[0,2])\n",
    "c.set_title(\"Histogram of BMI\",fontsize=14)\n",
    "d=sns.boxplot(y=\"Glucose\",data=pima,ax=axes[1,0])\n",
    "d.set_title(\"Boxplot of Glucose\",fontsize=14)\n",
    "e=sns.boxplot(y=\"BloodPressure\",data=pima,ax=axes[1,1])\n",
    "e.set_title(\"Boxplot of BloodPressure\",fontsize=14)\n",
    "f=sns.boxplot(y=\"BMI\",data=pima,ax=axes[1,2])\n",
    "f.set_title(\"Boxplot of BMI\",fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = dt_model.feature_importance\n",
    "x=pd.DataFrame(k*100,index=X_train.columns).sort_values(by=0,ascending=False)\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.barplot(x[0],x.index,palette='rainbow')\n",
    "plt.ylabel('Feature Name')\n",
    "plt.xlabel('Feature Importance in %')\n",
    "plt.title('Feature Importance Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(y_train,y_pred_train1),annot=True,fmt=\"d\",cbar=False,cmap=\"YlGnBu\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "\n",
    "df_cm = pd.DataFrame(data=confusion_matrix(Y_test,Y_test_pred),columns = [\"Predicted : 0\",\"Predicted : 1\"],index=[\"Actual : 0\",\"Actual : 1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"features\"] = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as SM\n",
    "def vif_cal(input_data):\n",
    "    x_vars=input_data\n",
    "    xvar_names=input_data.columns\n",
    "    for i in range(0,xvar_names.shape[0]):\n",
    "        y=x_vars[xvar_names[i]] \n",
    "        x=x_vars[xvar_names.drop(xvar_names[i])]\n",
    "        rsq=SM.ols(formula=\"y~x\", data=x_vars).fit().rsquared  \n",
    "        vif=round(1/(1-rsq),2)\n",
    "        print (xvar_names[i], \" VIF = \" , vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When transforming variables to log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"number_of_reviews\"] = np.where(df[\"number_of_reviews\"].isin([-np.inf,np.inf]),0,df[\"number_of_reviews\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will put this in a function..\n",
    "\n",
    "def lin_reg_model(X_features,y, \n",
    "                  return_coef = False):\n",
    "    \n",
    "    X = zomato_df[X_features]\n",
    "    # convert categorical to dummy\n",
    "    X = pd.get_dummies(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)\n",
    "    \n",
    "    # Training the model\n",
    "    regression_model = LinearRegression()\n",
    "    regression_model.fit(X_train, y_train)\n",
    "    \n",
    "    # predict\n",
    "    y_test_pred = regression_model.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    print(\"training r-square value is \",regression_model.score(X_train, y_train))\n",
    "    print(\"test r-square value is \", r2_score(y_test, y_test_pred))\n",
    "    print(\"rmse is \",np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "    \n",
    "    # return model coeff if flag is set to yes\n",
    "    if(return_coef):\n",
    "        ## extract coeff\n",
    "        model_coef = pd.DataFrame({\"features\":X_train.columns,\n",
    "                           \"coefficients\":regression_model.coef_})\n",
    "        return model_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier treatment using IQR technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capping_outliers(cont2_df):\n",
    "    def upper_range(k):\n",
    "        IQR1 = k.quantile(0.75) - k.quantile(0.25)\n",
    "        ur=k.quantile(0.75)+(1.5*IQR1)\n",
    "        return ur\n",
    "    \n",
    "    def lower_range(k):\n",
    "        IQR1 = k.quantile(0.75) - k.quantile(0.25)\n",
    "        ur=k.quantile(0.25)-(1.5*IQR1)\n",
    "        return lr\n",
    "    \n",
    "    for i in cont2_df.columns:\n",
    "        ur=upper_range(cont2_df[i])\n",
    "        lr=lower_range(cont2_df[i])\n",
    "        cont2_df[i]=np.where(cont2_df[i]>ur,ur,cont2_df[i])\n",
    "        cont2_df[i]=np.where(cont2_df[i]<lr,lr,cont2_df[i])\n",
    "    return cont2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For removing any elements from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_from_a_list(list,to_be_removed):\n",
    "    for i in to_be_removed:\n",
    "        list.remove(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Ols table to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_coef_table(lr_mod):\n",
    "  coef_table =pd.read_html(lr_mod.summary2().as_html())[1]\n",
    "  coef_table.drop(0,axis=0,inplace=True)\n",
    "  coef_table.columns = [\"var_name\",\"coef\",\"std_err\",\"t_stat\",\"p_val\",\"cof_25\",\"conf_975\"]\n",
    "  sig_features = list(coef_table.loc[coef_table[\"p_val\"].astype('float') <= 0.05, \"var_name\"].values)\n",
    "  sig_features = [x for x in sig_features if x != \"Intercept\"]\n",
    "  return sig_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CreditHistory.replace(to_replace='critical', value=0, inplace=True)\n",
    "df.CreditHistory.replace(to_replace='poor', value=1, inplace=True)\n",
    "df.CreditHistory.replace(to_replace='good', value=2, inplace=True)\n",
    "df.CreditHistory.replace(to_replace='verygood', value=3, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC & AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC and ROC for the training data\n",
    "\n",
    "\n",
    "# calculate AUC\n",
    "auc = roc_auc_score(Y_Test,y_test_predict_prob[:, 1])#keeping only the probabilities for the desired class outcome\n",
    "print('AUC: %.3f' % auc)\n",
    "# # calculate roc curve\n",
    "# from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(Y_Test,y_test_predict_prob[:, 1])#keeping only the probabilities for the desired \n",
    "#class outcome\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search and replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(\" \",\"_\",regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the datatype into number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"column name\"] = pd.to_numeric(df[\"column name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage in groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs=data.groupby([\"Ideal number of boys\",\"Blood sample\"]).count().reset_index()\n",
    "dfs=dfs.iloc[:,[0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_groupby(dfs):\n",
    "    percent=[]\n",
    "    for i in (dfs[dfs.columns[0]].unique()):\n",
    "        value0=dfs[(dfs[dfs.columns[0]]==i) & (dfs[dfs.columns[1]]==0)][dfs.columns[2]].reset_index().iloc[0,1]\n",
    "        value1=dfs[(dfs[dfs.columns[0]]==i) & (dfs[dfs.columns[1]]==1)][dfs.columns[2]].reset_index().iloc[0,1]\n",
    "        percent0=(value0/(value0+value1))*100\n",
    "        percent.append(percent0)\n",
    "        percent1=(value1/(value0+value1))*100\n",
    "        percent.append(percent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_sep(df):\n",
    "    numerical=[]\n",
    "    categorical=[]\n",
    "    for i in df.columns:\n",
    "        if df[i].dtypes==\"object\":\n",
    "            categorical.append(i)\n",
    "        else:\n",
    "            numerical.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns with null values greater than 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = sns.load_dataset(\"mpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_null_greater_than_80(df):\n",
    "    s=(df.isnull().sum()/df.shape[0])>80\n",
    "    return s[s].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note about datetime module: Two digits year ambiguity. So it seems that anything with the %y year below 69 will be attributed a century of 2000, and 69 upwards get 1900. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime.strptime('31-Dec-68', '%d-%b-%y').date()\n",
    ">>> datetime.date(2068, 12, 31)\n",
    "\n",
    "datetime.datetime.strptime('1-Jan-69', '%d-%b-%y').date()\n",
    ">>> datetime.date(1969, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code to fix this is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def fix_date(x):\n",
    "\n",
    "    if x.year >= 2040:\n",
    "\n",
    "        year = x.year - 100\n",
    "\n",
    "    else:\n",
    "\n",
    "        year = x.year\n",
    "\n",
    "    return datetime.date(year,x.month,x.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating categorical and numerical columns easy way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(df.dtypes.tolist()))\n",
    "df_num = df.select_dtypes(include=[\"float64\",\"int64\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cont_col_func(df):\n",
    "    return df.select_dtypes(include=[\"float64\",\"int64\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cat_col_func(df):\n",
    "    return df.select_dtypes(include=[\"O\"]).columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
